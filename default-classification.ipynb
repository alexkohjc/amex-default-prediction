{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nthis notebook will focus on optimizing the worst performing algorithm (Support Vectors) and only using 5K records for faster run-times\n\nafter hyperparam optimization:\n- f1-score: 0.80 -> 0.89\n- recall: 0.68 -> 0.74 (focusing on recall to reduce false negatives to catch more defaults;\nbut if biz objective is to increase # of clients who can get loans, we could focus on precision instead)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:48.681205Z","iopub.execute_input":"2023-09-21T05:34:48.681639Z","iopub.status.idle":"2023-09-21T05:34:48.690341Z","shell.execute_reply.started":"2023-09-21T05:34:48.681607Z","shell.execute_reply":"2023-09-21T05:34:48.688965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nimport shap\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\"\"\" OPTIONS - to not truncate rows/columns \"\"\"\npd.options.display.max_columns = None\npd.options.display.max_rows = 50\npd.options.display.max_colwidth = 20","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T05:34:48.692643Z","iopub.execute_input":"2023-09-21T05:34:48.693498Z","iopub.status.idle":"2023-09-21T05:34:48.710068Z","shell.execute_reply.started":"2023-09-21T05:34:48.693432Z","shell.execute_reply":"2023-09-21T05:34:48.708483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_vars = pd.read_csv('/kaggle/input/amex-default-prediction/train_data.csv', nrows=5_000)\ny_labels = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv', nrows=5_000)\n\ndf = X_vars.merge(y_labels, how='left', on='customer_ID', validate='m:1')\n\nassert ~df['target'].isna().any()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:48.711753Z","iopub.execute_input":"2023-09-21T05:34:48.712129Z","iopub.status.idle":"2023-09-21T05:34:49.047438Z","shell.execute_reply.started":"2023-09-21T05:34:48.712097Z","shell.execute_reply":"2023-09-21T05:34:49.046094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns=['customer_ID', 'S_2'])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.050982Z","iopub.execute_input":"2023-09-21T05:34:49.051531Z","iopub.status.idle":"2023-09-21T05:34:49.060414Z","shell.execute_reply.started":"2023-09-21T05:34:49.051484Z","shell.execute_reply":"2023-09-21T05:34:49.059238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_var = 'target'\ndf[target_var].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.062078Z","iopub.execute_input":"2023-09-21T05:34:49.062792Z","iopub.status.idle":"2023-09-21T05:34:49.076605Z","shell.execute_reply.started":"2023-09-21T05:34:49.062753Z","shell.execute_reply":"2023-09-21T05:34:49.075381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check distribution of target var\n# import seaborn as sns\n\n# sns.distplot(df[target_var])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.078353Z","iopub.execute_input":"2023-09-21T05:34:49.078740Z","iopub.status.idle":"2023-09-21T05:34:49.087272Z","shell.execute_reply.started":"2023-09-21T05:34:49.078710Z","shell.execute_reply":"2023-09-21T05:34:49.085998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"cat_cols = df.select_dtypes(include=['object', 'category']).columns.difference([target_var])\nprint(f'cat cols: {cat_cols}')\n\nnum_cols = df.select_dtypes(include=['int64', 'float64']).columns.difference([target_var])\nprint(f'num cols: {num_cols}')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.088800Z","iopub.execute_input":"2023-09-21T05:34:49.089938Z","iopub.status.idle":"2023-09-21T05:34:49.116986Z","shell.execute_reply.started":"2023-09-21T05:34:49.089896Z","shell.execute_reply":"2023-09-21T05:34:49.115487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_highly_correlated_vars(df, corr_thres=0.7):\n    \"\"\"\n    checks for groups of variables with high correlation, \n    and for each group, keep the var with highest correl to target variable\n    and delete the other variables\n    \n    but what if the selected var is then deleted because it also belongs to another set?\n    \n    \"\"\"\n    \n    X = df.drop(target_var, axis=1)\n    X_num = X[num_cols]\n    y = df[target_var]\n    \n    vars_to_keep = []\n    vars_to_drop = []\n    \n#     for col in X_num:\n#         print(col)\n#         corr_X_vars_w_col = X_num.drop(columns=col).corrwith(col, numeric_only=True).abs()\n#         print(corr_X_vars_w_col)\n\n\n    # check correlation\n    corr_matrix = df.corr(numeric_only=True).abs()\n\n#     # Get the upper triangle of the correlation matrix - a bit difficult to understand and hence troubleshoot\n#     upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n#     # Find index of feature columns with correlation greater than 0.7\n#     highly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.7)]\n\n    \n    for col in corr_matrix:\n        df_col = corr_matrix[col]\n        high_corr = df_col[(df_col>corr_thres) & (df_col<1)]\n        high_corr_lst = high_corr.index.tolist() + [col]\n        \n        if len(high_corr) > 1:\n#             print(col)\n#             print(high_corr_lst)\n\n#             for index, value in high_corr.items(): # corr of col with other cols\n#                 print(index, \":\", value)\n            \n            print()\n            \n            # get corr of highly correlated variables with target variable\n            corr_with_target = df[high_corr_lst].corrwith(y).abs()\n            print(corr_with_target)\n            \n            var_to_keep = corr_with_target.idxmax()\n            \n            vars_to_keep.extend([var_to_keep])\n            vars_to_drop.extend([corr_with_target.index.difference(var_to_keep)])\n            \n            print()\n\n    # for groups vars with high correl, delete and keep just the 1 var with highest corr with target var\n    \n    vars_to_keep = list(set(vars_to_keep))\n    vars_to_drop = list(set(vars_to_drop))\n    \n    return None\n\n# vars_to_keep = drop_highly_correlated_vars(df, corr_thres=0.7) ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.118622Z","iopub.execute_input":"2023-09-21T05:34:49.119102Z","iopub.status.idle":"2023-09-21T05:34:49.132668Z","shell.execute_reply.started":"2023-09-21T05:34:49.119067Z","shell.execute_reply":"2023-09-21T05:34:49.131343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # shift target variable to front\n# df_train = df_train[['target'] + [col for col in df_train.columns if col != 'target']]\n\n# correlation between variables \n# corr_with_target = df_train.corr()['target'].drop('target')\n# corr_with_target","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.134511Z","iopub.execute_input":"2023-09-21T05:34:49.134931Z","iopub.status.idle":"2023-09-21T05:34:49.149682Z","shell.execute_reply.started":"2023-09-21T05:34:49.134897Z","shell.execute_reply":"2023-09-21T05:34:49.147865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# feature engineering","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# modeling prep","metadata":{}},{"cell_type":"code","source":"model_df = df.copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.156615Z","iopub.execute_input":"2023-09-21T05:34:49.157110Z","iopub.status.idle":"2023-09-21T05:34:49.168975Z","shell.execute_reply.started":"2023-09-21T05:34:49.157076Z","shell.execute_reply":"2023-09-21T05:34:49.168078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def handle_missing_values(df, display_missing_vals, fill_mean_thres=0.1, drop_thres=0.9):\n    def handle_missing_num_cols(df):\n        if display_missing_vals:\n            # display missing values %\n            print('% missing values')\n            perc_na = df.isna().mean() * 100\n            print(perc_na[perc_na > 0].sort_values(ascending=False))\n\n        # drop cols with high % of missing values\n        missing_perc = df.drop(columns=target_var).isna().mean()\n\n        drop_cols = list(missing_perc[missing_perc > drop_thres].index)\n\n        print(f\"Dropping columns with more than {drop_thres*100}% missing values: {drop_cols}\\n\")\n        df = df.drop(columns=drop_cols)\n\n        # Fill missing values with column means for columns with less than 10% missing values\n        fill_mean_cols = list(missing_perc[(missing_perc <= fill_mean_thres) & (missing_perc > 0)].index)\n\n        print(f\"Filling columns with less than {fill_mean_thres*100}% missing values with their means: {fill_mean_cols}\\n\")\n        df[fill_mean_cols] = df[fill_mean_cols].fillna(df.mean())\n        \n        return df\n    \n    def handle_missing_cat_cols(df):\n        df[cat_cols] = df[cat_cols].fillna('None')\n        \n        return df\n    \n    \n    df = handle_missing_num_cols(df) \n    df = handle_missing_cat_cols(df)\n        \n    return df\n        \nmodel_df = handle_missing_values(model_df, fill_mean_thres=0.8, drop_thres=0.8, display_missing_vals=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.170482Z","iopub.execute_input":"2023-09-21T05:34:49.171291Z","iopub.status.idle":"2023-09-21T05:34:49.384500Z","shell.execute_reply.started":"2023-09-21T05:34:49.171244Z","shell.execute_reply":"2023-09-21T05:34:49.383236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scale_num_vars(df):\n    # scale numeric vals\n    from sklearn.preprocessing import StandardScaler\n\n    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.difference([target_var])\n    \n    scaler = StandardScaler()\n    num_df = df[num_cols]\n\n    scaler.fit(df[num_cols])\n    df_scaled = pd.DataFrame(scaler.transform(df[num_cols]), columns=num_cols)\n        \n    # concat back to original df\n    df_comb = pd.concat([df.drop(columns=num_cols), df_scaled], axis=1)\n    \n    assert df.shape == df_comb.shape\n    \n    return df_comb\n\ndef encode_cat_vars(df):\n    # encode cat vars\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n    df_shape = df.shape\n    cat_cols = df.select_dtypes(include=['object', 'category']).columns.difference([target_var])\n    \n    # using label encoder\n    label_encoders = {}\n    \n    for col in cat_cols:\n        label_encoders[col] = LabelEncoder() # one labelencoder for each col?\n        df[col] = label_encoders[col].fit_transform(df[col].values)\n    \n    \n    # using one-hot encoder - \n    \n    assert df.shape == df_shape\n    \n    return df\n\n# model_df = scale_num_vars(model_df) # cols are alr scaled\nmodel_df = encode_cat_vars(model_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.386098Z","iopub.execute_input":"2023-09-21T05:34:49.387181Z","iopub.status.idle":"2023-09-21T05:34:49.404210Z","shell.execute_reply.started":"2023-09-21T05:34:49.387144Z","shell.execute_reply":"2023-09-21T05:34:49.402876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = model_df.drop(columns=[target_var])\ny = model_df[target_var]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.406133Z","iopub.execute_input":"2023-09-21T05:34:49.407406Z","iopub.status.idle":"2023-09-21T05:34:49.418575Z","shell.execute_reply.started":"2023-09-21T05:34:49.407360Z","shell.execute_reply":"2023-09-21T05:34:49.417179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # use PCA to reduce collinearity\n# pca = PCA(n_components=5)\n# pca.fit(X)\n\n# # get the explained variance ratio\n# variance_ratio = pca.explained_variance_ratio_\n\n# # plot the scree plot\n# \"\"\" for scree plot, choose the # of vars when it starts to taper off \"\"\"\n# plt.plot(np.arange(1, len(variance_ratio)+1), variance_ratio, 'bo-', linewidth=2)\n# plt.xlabel('Principal Component')\n# plt.ylabel('Proportion of Variance Explained')\n# plt.title('Scree Plot')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.420468Z","iopub.execute_input":"2023-09-21T05:34:49.420874Z","iopub.status.idle":"2023-09-21T05:34:49.432842Z","shell.execute_reply.started":"2023-09-21T05:34:49.420843Z","shell.execute_reply":"2023-09-21T05:34:49.431576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca = PCA(n_components=2) # number of components to keep\n# X = pca.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.435097Z","iopub.execute_input":"2023-09-21T05:34:49.435452Z","iopub.status.idle":"2023-09-21T05:34:49.444207Z","shell.execute_reply.started":"2023-09-21T05:34:49.435424Z","shell.execute_reply":"2023-09-21T05:34:49.443359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the dataset into training and testing sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.445340Z","iopub.execute_input":"2023-09-21T05:34:49.445724Z","iopub.status.idle":"2023-09-21T05:34:49.462822Z","shell.execute_reply.started":"2023-09-21T05:34:49.445688Z","shell.execute_reply":"2023-09-21T05:34:49.461165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model fitting","metadata":{}},{"cell_type":"markdown","source":"# support vectors","metadata":{}},{"cell_type":"code","source":"# print('baseline model performance')\n# print('Support Vector Classifier: Accuracy Score =', accuracy_score(y_val, y_pred),\n#     ', F1 Score =', f1_score(y_val, y_pred),\n#     ', Recall =', recall_score(y_val, y_pred))#, average='weighted')) # using weighted average seems to give importance to the minority class, which is the class of interest\n\n\n# # focus\n# # 5K - Support Vector Classifier: Accuracy Score = 0.904 , F1 Score = 0.8095238095238094\n# # 5K (after removing date?) - Support Vector Classifier: Accuracy Score = 0.9 , F1 Score = 0.8023715415019763\n# # 5K (after hyperparam optimize): Test f1-score:  0.8870967741935483\n\n\n# # 10K - Support Vector Classifier: Accuracy Score = 0.8945 , F1 Score = 0.7965284474445516\n# # 20K - Support Vector Classifier: Accuracy Score = 0.893 , F1 Score = 0.7967711301044634","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.464621Z","iopub.execute_input":"2023-09-21T05:34:49.465082Z","iopub.status.idle":"2023-09-21T05:34:49.471303Z","shell.execute_reply.started":"2023-09-21T05:34:49.465040Z","shell.execute_reply":"2023-09-21T05:34:49.469994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create base svc model with standard params\n# not sure if this part is actually correct\nsvc = SVC(kernel='linear', C=1, random_state=42)\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nrecall_scores = cross_val_score(svc, X_train, y_train, cv=cv, scoring='recall')\nrecall = round(recall_scores.mean(), 2)\n\nprint(\"Cross-validation recall-score for base SVC model with K-fold cross-validation: \", recall)\n\n# fitting the SVC model with just the basic params\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_val)\n\n# f1 = f1_score(y_val, y_pred)#, average='weighted')\n# print(\"Test f1-score: \", f1)\n\nrecall = recall_score(y_val, y_pred)\nprint(\"Test recall score for base SVC model with K-fold cross-validation: \", recall)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:49.472818Z","iopub.execute_input":"2023-09-21T05:34:49.473578Z","iopub.status.idle":"2023-09-21T05:34:53.162080Z","shell.execute_reply.started":"2023-09-21T05:34:49.473544Z","shell.execute_reply":"2023-09-21T05:34:53.160598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model using confusion matrix\ncm = confusion_matrix(y_val, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=svc.classes_)\n\ndisp.plot()\nplt.show()\n\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:53.163758Z","iopub.execute_input":"2023-09-21T05:34:53.164145Z","iopub.status.idle":"2023-09-21T05:34:53.497190Z","shell.execute_reply.started":"2023-09-21T05:34:53.164114Z","shell.execute_reply":"2023-09-21T05:34:53.496011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# hyperpara optimization - hyperopt","metadata":{}},{"cell_type":"code","source":"# svc hyperpara optimization - hyperopt\n\"\"\"\ngridsearchCV is quite a brute-force way to optimize hyperparams as it explores all combinations of hyperparams\n\nhyperopt is generally better than gridsearchCV as it is more flexible and optimized for the search space, and can scale well with larger datasets\nand more complex models since it stops evaluating unpromising hyperparameter combinations much faster than GridSearchCV\n\"\"\"\n\n# define objective function with K-fold cross-validation to maximize recall score\ndef objective(space):\n    svc = SVC(C=space['C'], kernel=space['kernel'], gamma=space['gamma'])\n    cv = KFold(n_splits=5, shuffle=True)\n    scores = cross_val_score(svc, X_train, y_train, cv=cv, scoring='recall')\n    \n    return {'loss': -scores.mean(), 'status': STATUS_OK}\n#     f1 = f1_score(y_val, y_pred)#, average='weighted')\n#     return {'loss': -f1, 'status': STATUS_OK}\n\n# define the search space for hyperparameters\nsearch_space = {\n    'C': hp.uniform('C', 0.1, 10),\n    'kernel': hp.choice('kernel', ['linear', 'rbf', 'sigmoid']),\n    'gamma': hp.choice('gamma', ['auto', 'scale']),\n}\n\n# call the fmin function to optimize the objective function\nbest = fmin(objective, space=search_space, algo=tpe.suggest, max_evals=50, trials=Trials(), verbose=1)\n\n# print the best hyperparameters and test recall score\nprint(\"Best hyperparameters: \", best)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:34:53.498670Z","iopub.execute_input":"2023-09-21T05:34:53.499036Z","iopub.status.idle":"2023-09-21T05:38:05.195181Z","shell.execute_reply.started":"2023-09-21T05:34:53.499005Z","shell.execute_reply":"2023-09-21T05:38:05.193856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = space_eval(search_space, best)\n\n# fit the final model using best hyperparameters\nbest_svc = SVC(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'])\nbest_svc.fit(X_train, y_train)\ny_pred = best_svc.predict(X_val)\n\n# evaluate the model using K-fold cross-validation and compute f1 and recall score\n# f1_score = f1_score(y_val, y_pred)\n# print(\"Test f1-score using K-fold cross-validation and hyperopt: \", f1_score)\n\nrecall = recall_score(y_val, y_pred)\nprint(\"Test recall score using K-fold cross-validation and hyperopt: \", recall)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:38:05.196856Z","iopub.execute_input":"2023-09-21T05:38:05.197214Z","iopub.status.idle":"2023-09-21T05:38:05.943500Z","shell.execute_reply.started":"2023-09-21T05:38:05.197185Z","shell.execute_reply":"2023-09-21T05:38:05.942146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# svc hyperpara optimization - hyperopt\n\"\"\"\ngridsearchCV is quite a brute-force way to optimize hyperparams as it explores all combinations of hyperparams\n\nhyperopt is generally better than gridsearchCV as it is more flexible and optimized for the search space, and can scale well with larger datasets\nand more complex models since it stops evaluating unpromising hyperparameter combinations much faster than GridSearchCV\n\"\"\"\n\n# define objective function for hyperopt to optimize\ndef objective(space):\n    \"\"\"\n    This is the objective function that will be optimized using F1-score\n    \"\"\"\n    svc = SVC(C=space['C'], kernel=space['kernel'], gamma=space['gamma'])\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_val)\n#     f1 = f1_score(y_val, y_pred)#, average='weighted')\n#     return {'loss': -f1, 'status': STATUS_OK}\n\n    recall = recall_score(y_val, y_pred)\n    return {'loss': -recall, 'status': STATUS_OK}\n\n\n# define the search space for hyperopt\nsearch_space = {\n    'C': hp.uniform('C', 0.1, 10),\n    'kernel': hp.choice('kernel', ['linear', 'rbf', 'sigmoid']),\n    'gamma': hp.choice('gamma', ['auto', 'scale']),\n}\n\n# call the fmin function to optimize the objective function\nbest = fmin(objective, space=search_space, algo=tpe.suggest, max_evals=50, trials=Trials(), verbose=1)\n\n# print the best hyperparameters and test f1-score\nprint(\"Best hyperparameters: \", best)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:38:05.945164Z","iopub.execute_input":"2023-09-21T05:38:05.945618Z","iopub.status.idle":"2023-09-21T05:39:00.345944Z","shell.execute_reply.started":"2023-09-21T05:38:05.945574Z","shell.execute_reply":"2023-09-21T05:39:00.344498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = space_eval(search_space, best)\n\n# fit the final model using best hyperparameters and compute the f1-score on the test set\nbest_svc = SVC(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'])\nbest_svc.fit(X_train, y_train)\ny_pred = best_svc.predict(X_val)\n\nf1 = f1_score(y_val, y_pred)#, average='weighted')\nprint(\"Test f1-score: \", f1)\n\nrecall = recall_score(y_val, y_pred)\nprint(\"Test recall: \", recall)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:00.347466Z","iopub.execute_input":"2023-09-21T05:39:00.347856Z","iopub.status.idle":"2023-09-21T05:39:01.103209Z","shell.execute_reply.started":"2023-09-21T05:39:00.347825Z","shell.execute_reply":"2023-09-21T05:39:01.102130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# random forest","metadata":{}},{"cell_type":"code","source":"# Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42) # class_weight='balanced_subsample'\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:01.104955Z","iopub.execute_input":"2023-09-21T05:39:01.105849Z","iopub.status.idle":"2023-09-21T05:39:05.126806Z","shell.execute_reply.started":"2023-09-21T05:39:01.105802Z","shell.execute_reply":"2023-09-21T05:39:05.125660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Random Forest Classifier: Accuracy Score =', accuracy_score(y_val, y_pred),\n      ', F1 Score =', f1_score(y_val, y_pred))\n\n# 1K - Random Forest Classifier: Accuracy Score = 0.975 , F1 Score = 0.9425287356321839\n# 5K - Random Forest Classifier: Accuracy Score = 0.941 , F1 Score = 0.8822355289421159\n# 10K - Random Forest Classifier: Accuracy Score = 0.945 , F1 Score = 0.8917322834645669\n# 20K - Random Forest Classifier: Accuracy Score = 0.95025 , F1 Score = 0.9030686799805162","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:05.128205Z","iopub.execute_input":"2023-09-21T05:39:05.128547Z","iopub.status.idle":"2023-09-21T05:39:05.141495Z","shell.execute_reply.started":"2023-09-21T05:39:05.128518Z","shell.execute_reply":"2023-09-21T05:39:05.140418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model using confusion matrix\ncm = confusion_matrix(y_val, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=rf.classes_)\ndisp.plot()\nplt.show()\n\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:05.143695Z","iopub.execute_input":"2023-09-21T05:39:05.144036Z","iopub.status.idle":"2023-09-21T05:39:05.438540Z","shell.execute_reply.started":"2023-09-21T05:39:05.144007Z","shell.execute_reply":"2023-09-21T05:39:05.437419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"# XGBoost Classifier\nxgb = XGBClassifier(random_state=42) # scale_pos_weight=sum(target==0)/sum(target==1), \nxgb.fit(X_train, y_train)\n\ny_pred = xgb.predict(X_val)\nprint('XGBoost Classifier: Accuracy Score =', accuracy_score(y_val, y_pred),\n      ', F1 Score =', f1_score(y_val, y_pred))\n\n# 5K - XGBoost Classifier: Accuracy Score = 0.982 , F1 Score = 0.96484375\n# 10K - XGBoost Classifier: Accuracy Score = 0.9755 , F1 Score = 0.9524733268671194\n# 20K - XGBoost Classifier: Accuracy Score = 0.9755 , F1 Score = 0.953199617956065","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:05.440028Z","iopub.execute_input":"2023-09-21T05:39:05.440501Z","iopub.status.idle":"2023-09-21T05:39:09.918489Z","shell.execute_reply.started":"2023-09-21T05:39:05.440469Z","shell.execute_reply":"2023-09-21T05:39:09.917500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the model using confusion matrix\ncm = confusion_matrix(y_val, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=xgb.classes_)\ndisp.plot()\nplt.show()\n\nprint(cm)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:09.925789Z","iopub.execute_input":"2023-09-21T05:39:09.926609Z","iopub.status.idle":"2023-09-21T05:39:10.226559Z","shell.execute_reply.started":"2023-09-21T05:39:09.926566Z","shell.execute_reply":"2023-09-21T05:39:10.225410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cr = classification_report(y_val, y_pred)\nprint(cr)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:10.228373Z","iopub.execute_input":"2023-09-21T05:39:10.229271Z","iopub.status.idle":"2023-09-21T05:39:10.247088Z","shell.execute_reply.started":"2023-09-21T05:39:10.229235Z","shell.execute_reply":"2023-09-21T05:39:10.245716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# xgboost - hyperparams tuning","metadata":{}},{"cell_type":"code","source":"# import xgboost as xgb\n# from sklearn.datasets import load_iris\n# from sklearn.model_selection import train_test_split\n# from hyperopt import fmin, hp, tpe, Trials\n# from sklearn.metrics import accuracy_score\n\n# # load iris dataset\n# iris = load_iris()\n# X = iris.data\n# y = iris.target\n\n# # split the data into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # define the search space for Hyperopt\n# space = {\n#     'max_depth': hp.choice('max_depth', range(1, 10)),\n#     'learning_rate': hp.loguniform('learning_rate', -5, 0),\n#     'n_estimators': hp.choice('n_estimators', range(50, 500)),\n#     'gamma': hp.loguniform('gamma', -5, 0),\n#     'min_child_weight': hp.choice('min_child_weight', range(1, 10)),\n#     'subsample': hp.uniform('subsample', 0.1, 1),\n#     'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1),\n#     'reg_alpha': hp.loguniform('reg_alpha', -5, 5),\n#     'reg_lambda': hp.loguniform('reg_lambda', -5, 5),\n#     'objective': 'multi:softmax',\n#     'num_class': 3\n# }\n\n# # define the objective function for Hyperopt\n# def objective(params):\n#     model = xgb.XGBClassifier(**params)\n#     model.fit(X_train, y_train)\n#     y_pred = model.predict(X_test)\n#     accuracy = accuracy_score(y_test, y_pred)\n#     return {'loss': 1-accuracy, 'status': 'ok'}\n\n# # set the number of trials for Hyperopt\n# trials = Trials()\n\n# # run the Hyperopt search\n# best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n# print(\"Best Hyperparameters:\", best)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:10.248505Z","iopub.execute_input":"2023-09-21T05:39:10.248909Z","iopub.status.idle":"2023-09-21T05:39:10.256914Z","shell.execute_reply.started":"2023-09-21T05:39:10.248876Z","shell.execute_reply":"2023-09-21T05:39:10.255576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check if over or underfitting","metadata":{}},{"cell_type":"code","source":"# make predictions on the train and test sets\ny_pred_train = xgb.predict(X_train)\n\n# evaluate the performance of the model\ntrain_accuracy = accuracy_score(y_train, y_pred_train)\nval_accuracy = accuracy_score(y_val, y_pred)\nprint(\"Train Accuracy: %.2f%%\" % (train_accuracy * 100.0))\nprint(\"Val Accuracy: %.2f%%\" % (val_accuracy * 100.0))\nprint()\n\ntrain_f1_score = f1_score(y_train, y_pred_train)\nval_f1_score = f1_score(y_val, y_pred)\nprint(\"Train f1 score: %.2f%%\" % (train_f1_score * 100.0))\nprint(\"Val f1 score: %.2f%%\" % (val_f1_score * 100.0))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:10.258642Z","iopub.execute_input":"2023-09-21T05:39:10.259104Z","iopub.status.idle":"2023-09-21T05:39:10.303674Z","shell.execute_reply.started":"2023-09-21T05:39:10.259071Z","shell.execute_reply":"2023-09-21T05:39:10.302563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import mean_squared_error, r2_score\n\n# # Train the XGBoost regressor model and plot the learning curve\n# train_error = []\n# test_error = []\n# n_rounds = 100\n\n# for i in range(n_rounds):\n#     xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\n#     y_train_pred = xgb.predict(X_train)\n#     y_val_pred = xgb.predict(X_val)\n#     train_error.append(mean_squared_error(y_train, y_train_pred))\n#     test_error.append(mean_squared_error(y_val, y_val_pred))\n    \n# plt.plot(range(1, n_rounds+1), train_error, label='Training error')\n# plt.plot(range(1, n_rounds+1), test_error, label='Validation error')\n# plt.xlabel('Number of iterations')\n# plt.ylabel('MSE')\n# plt.legend(loc='best')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:10.306783Z","iopub.execute_input":"2023-09-21T05:39:10.307288Z","iopub.status.idle":"2023-09-21T05:39:10.313052Z","shell.execute_reply.started":"2023-09-21T05:39:10.307245Z","shell.execute_reply":"2023-09-21T05:39:10.311733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# xgboost feature importance","metadata":{}},{"cell_type":"code","source":"# get importance\nimportance = xgb.feature_importances_\n\nfeat_impt_df = pd.DataFrame({\n    'feature': X.columns.tolist(),\n    'feature_importance': importance}).sort_values(by='feature_importance', ascending=False)\n\nfeat_impt_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:10.314643Z","iopub.execute_input":"2023-09-21T05:39:10.315442Z","iopub.status.idle":"2023-09-21T05:39:10.338824Z","shell.execute_reply.started":"2023-09-21T05:39:10.315396Z","shell.execute_reply":"2023-09-21T05:39:10.337517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# xgboost shap values","metadata":{}},{"cell_type":"code","source":"# create a SHAP explainer object\nexplainer = shap.Explainer(xgb)\n\n# calculate SHAP values for all features\nshap_values = explainer(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:10.340372Z","iopub.execute_input":"2023-09-21T05:39:10.340734Z","iopub.status.idle":"2023-09-21T05:39:11.342175Z","shell.execute_reply.started":"2023-09-21T05:39:10.340699Z","shell.execute_reply":"2023-09-21T05:39:11.341194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values)#, max_display=999)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:11.343764Z","iopub.execute_input":"2023-09-21T05:39:11.344541Z","iopub.status.idle":"2023-09-21T05:39:13.651512Z","shell.execute_reply.started":"2023-09-21T05:39:11.344504Z","shell.execute_reply":"2023-09-21T05:39:13.650573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(-1, -21, -1):\n#     var = shap_values[:, shap_values.abs.mean(0).argsort[i]]\n#     # var = shap_values[:, 'Jerneh_Occupation_grp']\n    \n#     shap.plots.scatter(var, color=shap_values,\n#         # xmin=var.percentile(1), xmax=var.percentile(99),\n#         ymin=var.percentile(1), ymax=var.percentile(99),\n#         # ymin=-0.1, ymax=0.1,\n#         alpha=0.2,\n#         # dot_size=2,\n#         # x_jitter=0.1,\n        \n#     )","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:13.653041Z","iopub.execute_input":"2023-09-21T05:39:13.653997Z","iopub.status.idle":"2023-09-21T05:39:13.659166Z","shell.execute_reply.started":"2023-09-21T05:39:13.653962Z","shell.execute_reply":"2023-09-21T05:39:13.657840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import mlflow","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:13.660396Z","iopub.execute_input":"2023-09-21T05:39:13.661270Z","iopub.status.idle":"2023-09-21T05:39:13.676809Z","shell.execute_reply.started":"2023-09-21T05:39:13.661221Z","shell.execute_reply":"2023-09-21T05:39:13.675862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def amex_metric(y_true, y_pred):\n#     labels = np.transpose(np.array([y_true, y_pred]))\n#     labels = labels[labels[:, 1].argsort()[::-1]]\n#     weights = np.where(labels[:,0]==0, 20, 1)\n#     cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n#     top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n#     gini = [0,0]\n\n#     for i in [1,0]:\n#         labels = np.transpose(np.array([y_true, y_pred]))\n#         labels = labels[labels[:, i].argsort()[::-1]]\n#         weight = np.where(labels[:,0]==0, 20, 1)\n#         weight_random = np.cumsum(weight / np.sum(weight))\n#         total_pos = np.sum(labels[:, 0] *  weight)\n#         cum_pos_found = np.cumsum(labels[:, 0] * weight)\n#         lorentz = cum_pos_found / total_pos\n#         gini[i] = np.sum((lorentz - weight_random) * weight)\n    \n#     return 0.5 * (gini[1]/gini[0] + top_four)\n\n# amex_metric(y_val, y_pred_xgb)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:13.678486Z","iopub.execute_input":"2023-09-21T05:39:13.678934Z","iopub.status.idle":"2023-09-21T05:39:13.690282Z","shell.execute_reply.started":"2023-09-21T05:39:13.678894Z","shell.execute_reply":"2023-09-21T05:39:13.689374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from hyperas.distributions import choice, uniform\n\n# # TODO: how do i run hyperopt on multiple models, if i don't want to let the model architecture be part of the \n# # 'tuning' per se?\n\n# keep_loss = []\n\n# space = {\n#     'activation': choice('activation', ['relu','sigmoid']),\n#     'dropout': uniform('dropout', 0, 1),\n#     'batch_size': choice('batch_size', [64, 128]),\n#     'learning_rate': uniform('learning_rate', 0.01, 1),\n#     'clip_norm': uniform('clip_norm', 0.01, 1),\n#     'num_nodes_input': choice('num_nodes_input', [100, 300, 500]),\n#     'num_nodes_hidden': choice('num_nodes_hidden', [50, 150, 250]),\n#     'num_nodes_output': choice('num_nodes_output', [25, 75, 150]),\n#     'num_hidden_layers': uniform('num_hidden_layers', 1, 7),\n#     # TODO: should we tune the # of epochs as well?\n#     # 'num_epochs': uniform('num_epochs', 10, 100),\n# }\n\n# def objective_func(space_sample):\n\n#     ## parse the hyper-parameter sample\n#     activ = space_sample['activation']\n#     dropout = space_sample['dropout']\n#     size = space_sample['batch_size']\n#     rate = space_sample['learning_rate']\n#     clip_norm = space_sample['clip_norm']\n#     input = space_sample['num_nodes_input']\n#     hidden = space_sample['num_nodes_hidden']\n#     output = space_sample['num_nodes_output']\n#     hl = round(space_sample['num_hidden_layers'])\n#     epochs = 50\n\n#     model = Sequential()\n\n#     model.add(Dense(input, input_shape=(X_train.shape[1],), activation=activ)) # the input_shape has to be same dim as the vectors\n#     model.add(Dropout(dropout))\n\n#     for i in range(hl):\n#         model.add(Dense(hidden, activation=activ))\n#         model.add(Dropout(dropout))\n\n#     # TODO: maybe can add a batch normalization layer before the output layer?\n\n#     model.add(Dense(output, activation=None)) \n#     model.add(Lambda(lambda x: tf.math.l2_normalize(x, axis=1))) # L2 normalize embeddings\n\n#     from keras.callbacks import EarlyStopping, TerminateOnNaN, ModelCheckpoint\n    \n#     early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=7)\n#     terminate = TerminateOnNaN()\n\n#     model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', \n#         save_best_only=True, verbose=0)\n\n#     model.compile(\n#             optimizer=tf.keras.optimizers.Adam(learning_rate=rate, clipnorm=clip_norm),\n#             loss=tfa.losses.TripletSemiHardLoss(),\n#         )\n        \n#     history = model.fit(\n#             X_train, \n#             y_train, \n#             validation_data=(X_test, y_test), \n#             batch_size=size, \n#             epochs=epochs,\n#             callbacks=[early_stop, terminate, model_checkpoint],\n#             verbose=0\n#         )        \n\n#     plot_train_valid_loss(history)\n\n#     '''\n#     - for Siamese Network, running the model (i.e. .predict()) on X_val will not give us a 'prediction'/y-label,\n#     unlike in a conventional ML/NN model\n    \n#     - what it provides instead is the 'embedding' of X_val in the 'embedding space' created by the model (think\n#     of it as the 'location' of X_val in a 'transformed space')\n    \n#     - therefore we can't directly assign a label to the records in X_val; what we'll do instead is to:\n#         - transform X_train into the same embedding space \n#         - compare each record of X_val to each record of X_train\n#         - find the closest (min. euclidean dist) btw the 2 and use that as the loss (TBC)\n#         - to calculate accuracy:\n#         - (assign the label of that closest X_train record as the label for the X_val record)\n#     '''\n\n#     # or should i get the min or avg val_loss from history and use that as the loss?\n#     # TODO: X_test or X_val?\n    \n#     def calc_min_euc_dist():\n#         from scipy.spatial import distance\n\n#         X_train_embed = history.model.predict(X_train)\n#         X_val_embed = history.model.predict(X_val)\n#         assert len(X_train_embed)==len(X_train)\n#         assert len(X_val_embed)==len(X_val)\n\n#         min_dist_all = [] # TODO: consider renaming this var\n\n#         for embed_x_val in X_val_embed:\n#             min_dist = []\n\n#             for embed_x_train in X_train_embed:\n#                 # calc all pairwise distances\n#                 euc_dist = distance.euclidean(embed_x_train, embed_x_val)\n#                 min_dist.append(euc_dist)\n            \n#             assert len(min_dist)==len(X_train)\n\n#             min_dist = min(min_dist)\n\n#             min_dist_all.append(min_dist)\n\n#             # TODO: consider adding in y-label prediction for accuracy scoring?\n\n#         assert len(min_dist_all) == len(X_val_embed)\n\n#         # TODO: check if taking the sum of min_dist as the loss is correct\n#         min_euc_dist = np.sum(min_dist_all)\n#         print(min_euc_dist)\n#         assert min_euc_dist!=0, keep_loss.append([X_train_embed, X_val_embed])\n\n#         return min_euc_dist\n    \n#     loss = calc_min_euc_dist()\n\n#     # loss = np.min(history.history['val_loss']) # prev method\n\n#     return loss","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:39:13.692119Z","iopub.execute_input":"2023-09-21T05:39:13.692506Z","iopub.status.idle":"2023-09-21T05:39:13.708105Z","shell.execute_reply.started":"2023-09-21T05:39:13.692474Z","shell.execute_reply":"2023-09-21T05:39:13.706147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_function(keep_loss):\n\n    X_train_embed = keep_loss[0][0]\n    print(len(X_train_embed))\n\n    X_val_embed = keep_loss[0][1]\n    print(len(X_val_embed))\n\n    return None\n\n# test_function(keep_loss)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:40:05.610097Z","iopub.execute_input":"2023-09-21T05:40:05.610489Z","iopub.status.idle":"2023-09-21T05:40:05.616257Z","shell.execute_reply.started":"2023-09-21T05:40:05.610459Z","shell.execute_reply":"2023-09-21T05:40:05.614738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # load a saved model\n# from keras.models import load_model\n# saved_model = load_model('best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:40:07.184054Z","iopub.execute_input":"2023-09-21T05:40:07.184474Z","iopub.status.idle":"2023-09-21T05:40:07.189506Z","shell.execute_reply.started":"2023-09-21T05:40:07.184437Z","shell.execute_reply":"2023-09-21T05:40:07.188399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def assign_y_label(history, X_train, y_train, X_val, n=1): \n    '''\n    - for Siamese Network, running the model (i.e. .predict()) on X_val will not give us a 'prediction'/y-label,\n    unlike in a conventional ML/NN model\n    \n    - what it provides instead is the 'embedding' of X_val in the 'embedding space' created by the model (think\n    of it as the 'location' of X_val in a 'transformed space')\n    \n    - therefore we can't directly assign a label to the records in X_val; what we'll do instead is to:\n        - transform X_train into the same embedding space \n        - compare each record of X_val to each record of X_train\n        - find the closest (min. euclidean dist) btw the 2\n        - assign the label of that closest X_train record as the label for the X_val record\n\n    - smth about looking at N=3 closest...\n\n\n    '''\n    from scipy.spatial import distance\n\n    X_val_embed = history.model.predict(X_val)\n    X_train_embed = history.model.predict(X_train)\n\n    y_pred = [] \n\n    for embed_x_val in X_val_embed:\n        row_dist = []\n        \n        for embed_x_train in X_train_embed:\n            # calc all pairwise distances\n            euc_dist = distance.euclidean(embed_x_train, embed_x_val)\n            row_dist.append(euc_dist)\n\n        row_dist = np.array(row_dist)\n        n_min_dist_idx = np.argpartition(row_dist, n)[:n] # returns indexs of N min dist\n        y_pred.append(y_train[n_min_dist_idx]) # \n    \n    y_pred = np.array(y_pred)\n\n    assert len(y_pred) == len(X_val_embed)\n    assert y_pred.shape[1] == n\n\n    return y_pred\n\n# y_pred = assign_y_label(history, X_train, y_train, X_val, n=5)\n# note: for a single row in y_pred, there might be multiple of the same class","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:40:08.505450Z","iopub.execute_input":"2023-09-21T05:40:08.506740Z","iopub.status.idle":"2023-09-21T05:40:08.517937Z","shell.execute_reply.started":"2023-09-21T05:40:08.506694Z","shell.execute_reply":"2023-09-21T05:40:08.516714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # visualize with tensorboard\n# # load into https://projector.tensorflow.org/\n\n# # Save test embeddings for visualization in projector\n# np.savetxt(\"vecs.tsv\", y_pred, delimiter='\\t')\n\n# out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n\n# # for img, labels in tfds.as_numpy(test_dataset):\n# #     [out_m.write(str(x) + \"\\n\") for x in labels]\n\n# for x in y_test_inversed:\n#   out_m.write(str(x) + \"\\n\")\n\n# out_m.close()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T05:40:09.764690Z","iopub.execute_input":"2023-09-21T05:40:09.765127Z","iopub.status.idle":"2023-09-21T05:40:09.770869Z","shell.execute_reply.started":"2023-09-21T05:40:09.765093Z","shell.execute_reply":"2023-09-21T05:40:09.769322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}